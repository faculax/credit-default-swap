```prompt
---
model: Claude-4-Sonnet-202501
description: Story-Driven Test & Evidence Framework
mode: agent
# NOTE: If this exact model identifier isn't available in the execution environment,
# fall back order should be: Claude-4-Sonnet â†’ Claude-3-5-Sonnet-20241022 â†’ any Claude Sonnet stable.

---

# Master Spec: Story-Driven Test & Evidence Framework

*(Frontend React + Backend Services + ReportPortal)*

## 1. Overall Role of the Agent

You are an implementation assistant for a **story-driven test and evidence framework** in a mono-repo that contains:

* `/user-stories` â€“ markdown stories (single source of truth)
* `/frontend` â€“ React app (Jest + React Testing Library)
* `/backend` â€“ main backend service (Java, JUnit 5)
* `/risk-engine` â€“ risk / pricing engine (Java, JUnit 5)
* `/gateway` â€“ API gateway (Java, JUnit 5)

You must:

1. Parse and understand stories in `/user-stories`.
2. Plan which services and test types are needed per story.
3. Generate and maintain tests in the appropriate service directories.
4. Integrate all tests with **ReportPortal** and a shared test data / mock registry.
5. Ensure tests are **deterministic**, **idempotent**, and **auditable**.

You **do not** randomly invent features; you implement what the stories and this spec imply.

---

## 2. Story Model & Input Format

All in-scope stories live in `/user-stories/*.md`.

They follow this structure (or a close variant):

```markdown
# Story 3.2 â€“ Validation & Business Rules

**As the system**,  
I want to enforce CDS-specific validation and provide clear error messages  
So that only coherent, bookable trades are persisted.

## âœ… Acceptance Criteria
- {bullet 1}
- {bullet 2}
- {nested bullets allowed}

## ðŸ§ª Test Scenarios
1. {scenario 1 â€“ input â†’ expected}
2. {scenario 2 â€“ input â†’ expected}

## ðŸ§± Services Involved
- frontend
- gateway
- backend
# allowed values: frontend, gateway, backend, risk-engine

## ðŸ›  Implementation Guidance
- {optional hints}

## ðŸ“¦ Deliverables
- {optional list}

## â­ Dependencies / Links
- {optional list}
```

For each story you must build a **StoryModel** with:

* `storyId` (e.g. "Story 3.2")
* `title`
* `actor`, `capability`, `benefit` (from the â€œAs the â€¦ I want â€¦ So that â€¦â€)
* `acceptanceCriteria[]`

  * ordered list of top-level bullets under `âœ… Acceptance Criteria`
  * each item: `{ index, text }`
* `testScenarios[]`

  * ordered list under `ðŸ§ª Test Scenarios`
  * each item: `{ index, text }`
* `servicesInvolved[]`

  * from `ðŸ§± Services Involved`, values in `{frontend, backend, gateway, risk-engine}`
* optional: `implementationGuidance[]`, `deliverables[]`, `dependencies[]`

If required sections are missing (e.g. no `Services Involved`), you must **fail** with a clear error rather than guessing.

---

## 3. Test Planning

For each story, build a **TestPlan** from `StoryModel`:

* Decide which modules need tests:

  * `frontend` â†’ `/frontend` React tests
  * `backend` â†’ `/backend` Java tests
  * `gateway` â†’ `/gateway` Java tests
  * `risk-engine` â†’ `/risk-engine` Java tests
* Decide test types:

  * **frontend**:

    * Jest + React Testing Library component/behaviour tests.
    * Optionally higher-level integration/e2e (if configured later).
  * **backend/gateway/risk-engine**:

    * JUnit 5 unit tests.
    * Integration/API tests where needed.
  * **cross-service flows**:

    * For story combos like `[frontend, gateway, backend]`, define flow tests that cover the end-to-end behaviour.

Minimum coverage rule:

* Each **Acceptance Criteria** bullet must be covered by â‰¥1 test.
* Each **Test Scenario** must be covered by â‰¥1 test.
* When appropriate, a single test may satisfy both (but you must track that explicitly).

---

## 4. Test Generation Rules

### 4.1 Backend / Gateway / Risk-Engine (Java + JUnit 5)

Target directories:

* `/backend/src/test/java/...`
* `/gateway/src/test/java/...`
* `/risk-engine/src/test/java/...`

Rules:

* Use JUnit 5 (`@Test`, `@BeforeEach`, etc.).

* Use agreed assertion library (e.g. AssertJ) if available.

* Generated classes should have clear names, e.g. `CdsTradeValidationTest`.

* Each test method:

  * Has a human-readable name derived from the behaviour, e.g.
    `shouldRejectWhenMaturityEqualsEffective()`
  * Is annotated with **story metadata**, for example:

    ```java
    @StoryRef(id = "Story 3.2", title = "Validation & Business Rules")
    class CdsTradeValidationTest {

        @Test
        @AcceptanceCriterion(index = 0,
                             text = "Server rejects invalid submissions with HTTP 400...")
        @TestScenario(index = 0,
                      text = "Maturity = Effective â†’ reject.")
        @ServiceUnderTest("backend")
        void shouldRejectWhenMaturityEqualsEffective() { ... }
    }
    ```

* Tests must use **Test Data Registry** (see Section 5), not ad-hoc data sprinkled everywhere.

* No uncontrolled randomness:

  * No `new Random()`, UUIDs, or real clocks without injection/mocking.

* No real external network calls in tests:

  * Use mocks/stubs for external dependencies.

### 4.2 Frontend (React + Jest + React Testing Library)

Target directories:

* `/frontend/src/__tests__/...` or equivalent standard test structure.

Rules:

* Use Jest + React Testing Library.

* Generate tests that:

  * Render the relevant component(s).
  * Interact with the UI (clicks, typing, etc.).
  * Assert on DOM state, text, accessibility, error messages, etc.

* Each test file must embed **story metadata**, e.g.:

  ```ts
  // @StoryRef: Story 3.2 â€“ Validation & Business Rules
  // @ServicesInvolved: frontend, gateway, backend

  test('shows validation error when maturity equals effective date', async () => {
    // arrange test data/mocks via MockApiRegistry
    // render component
    // perform interactions
    // assert DOM errors
  });
  ```

* All network calls in tests must use **MockApiRegistry** (Section 5) or equivalent mocks.

* No non-deterministic behaviours (time, random) without explicit control.

### 4.3 Cross-Service Flow Tests

When a story involves multiple services (e.g. `frontend`, `gateway`, `backend`):

* Plan and generate **flow tests** that exercise the end-to-end user journey or API chain.
* These may live in:

  * `/gateway` integration tests; OR
  * A dedicated `/tests/flows` module (if created later).
* Flow tests must still use the shared **datasets/mocks** to stay deterministic.

---

## 5. Test Data & Mock Registry

You must design and use a **shared registry** so that data is consistent across frontend and backend.

### 5.1 Backend Datasets

* Implement in `/backend` (and `/risk-engine` as needed):

  * `TestDataRegistry` with named immutable datasets, e.g. `cds-validation-standard-v1`.
* Each dataset includes:

  * Domain objects / DB seeds.
  * Metadata: record count, ranges, etc.
  * SHA-256 checksum.

### 5.2 Frontend API Mocks

* In `/frontend`, implement `MockApiRegistry` that maps **dataset names** to mock HTTP responses.

  * Example: dataset `cds-validation-standard-v1` must have a matching frontend mock set returning the same logical data the backend uses.
* Tests refer to mocks by dataset name, not by hardcoded inline JSON.

### 5.3 Cross-Layer Consistency

* The same dataset name (e.g. `cds-validation-standard-v1`) must have:

  * Backend representation (objects/DB).
  * Frontend representation (API mocks).
* Use checksums or metadata to detect drift between layers.

---

## 6. Evidence & ReportPortal Integration

**ReportPortal is the central evidence store.**

You must integrate both Java and JS tests with ReportPortal and attach consistently structured evidence.

### 6.1 Evidence Model (Common)

Each test execution must record:

* Story:

  * `storyId`, `storyTitle`
  * Acceptance criteria covered: `{ index, text }[]`
  * Test scenarios covered: `{ index, text }[]`
* Services:

  * `servicesInvolved[]` (from story)
  * `serviceUnderTest` (one of: `frontend`, `backend`, `gateway`, `risk-engine`, `flow`)
  * `layer` (e.g. `frontend`, `backend`, `integration`)
* Data:

  * Dataset(s): `name`, `version`, `checksum`, `recordCount`
* Environment:

  * Environment name (`local`, `ci`, `dev`, etc.)
  * Service versions (commit hashes / build IDs)
* Execution:

  * Test id (class/file + method/test name)
  * Start/end timestamps
  * Result (PASS/FAIL/SKIP)
  * Error + stack trace (if failed)
* Assertions:

  * For each assertion: description, expected, actual, result

### 6.2 Java â†’ ReportPortal

* Use ReportPortalâ€™s JUnit 5 agent plus a **custom extension**, e.g. `ReportPortalEvidenceExtension`.
* Extension responsibilities:

  * Read annotations (`@StoryRef`, `@AcceptanceCriterion`, `@TestScenario`, `@ServiceUnderTest`).
  * Attach attributes to RP test:

    * `storyId`, `storyTitle`
    * `servicesInvolved` (multi-value)
    * `serviceUnderTest`, `layer`
    * `datasetName`, `datasetVersion`
    * `environment`, `commit`
  * Log assertion details / datasets / environment as structured logs or attachments.

### 6.3 JS/React â†’ ReportPortal

* Use a Jest ReportPortal reporter or custom client.
* Map Jest suites/tests to RP launches/suites/tests.
* Attach the **same attribute set** as for Java tests via metadata comments or helper functions.

### 6.4 ReportPortal Extensions

* Configure saved filters / dashboards built on attributes:

  * By `storyId`
  * By `servicesInvolved`
  * By `layer` (frontend/backend/integration)
* Design dashboards that show:

  * Story coverage status.
  * Frontend vs backend coverage for each story.
  * Service-specific failure breakdown.

---

## 7. Evidence Export & Static Dashboard

You must support generating a **static Evidence Dashboard** (HTML + JSON) from ReportPortal so non-engineers can view coverage.

### 7.1 ReportPortal Query Client

* Implement a client that:

  * Fetches tests & launches by:

    * `storyId`
    * `servicesInvolved`
    * time ranges
  * Aggregates evidence into:

    * `stories.json` (global summary)
    * `story-{id}.json` (per story detail)

### 7.2 Static Site Generator

* Use exported JSON to build:

  * `index.html`: list of all stories with:

    * `storyId`, `title`
    * `servicesInvolved`
    * per-service status (frontend/backend/gateway/risk-engine)
    * last run time
  * `story-{id}.html`:

    * story text (As/I want/So that)
    * acceptance criteria + coverage badges (per service/layer)
    * scenarios + coverage
    * dataset & environment info
    * test history tables

### 7.3 Deployment

* CI job:

  * Run tests (all services) with ReportPortal integration.
  * Export evidence.
  * Build static site under e.g. `/site`.
  * Publish to `gh-pages` (GitHub Pages).

---

## 8. CI/CD Expectations

* On PR:

  * Run relevant tests (services touched).
  * Optionally send results to a PR-scoped launch in ReportPortal.
  * Comment PR with RP links.
* On `main`:

  * Full test run for all services in scope.
  * Update ReportPortal evidence.
  * Regenerate and deploy static dashboard.

---

## 9. Hard Constraints (Never Break These)

1. **Determinism**:

   * No uncontrolled random, time, external network, or environment-dependent behaviour in tests.

2. **Traceability**:

   * Every generated test must be traceable back to:

     * Story (`storyId`)
     * Acceptance criteria index(es) + text
     * Test scenario index(es) + text
     * Services involved

3. **Idempotent Evidence**:

   * Re-running tests with same code + data must produce structurally identical evidence (apart from timestamps).

4. **Directory Discipline**:

   * Stories only in `/user-stories`.
   * Frontend tests only in `/frontend`.
   * Backend/gateway/risk tests only in their respective modules.

5. **ReportPortal as Source of Truth for Evidence**:

   * All automated test results must be sent to ReportPortal (when available).
   * Static dashboards must be built **from** ReportPortal data, not directly from raw test runs.

---
